# Major-Project
## Project Concept: AI-Powered Smart Car for Gesture & Voice-Controlled Wheelchair
This project involves building a toy car prototype that can be controlled using hand gestures, voice commands, or autonomous navigation, demonstrating its potential for smart wheelchairs.

### Key Features & Technologies:
- Hand Gesture Control:
Uses ESP32-CAM + OpenCV + Mediapipe to recognize hand gestures.
Different gestures (e.g., forward, backward, stop) control the car's movement.
- Voice-Controlled Navigation:
Uses ESP32 + Google Speech-to-Text API or Edge AI (Vosk, Picovoice).
Recognizes voice commands like "Move forward," "Turn left," etc.
- Obstacle Avoidance & Safety Mechanism:
Uses Ultrasonic Sensors / LiDAR to detect obstacles and stop automatically.
Can integrate AI-based object detection (YOLO) for recognizing humans/objects.
- IoT-Based Remote Control:
Connects with a mobile app or web dashboard (Firebase, MQTT, Blynk) for remote control.
- Self-Navigation Mode:
Uses Reinforcement Learning (Q-learning, Deep Q-Networks) for autonomous movement.
Can follow a pre-defined path using color detection (line following).
### Future Expansion into a Wheelchair:
The same technology can be scaled up for real wheelchairs with motorized movement and smart safety features.
Can add fall detection, emergency alerts, and AI-based obstacle recognition.

----------------------------------------------------------------------------------------------------------------------------------
## üöó Architecture Overview
The system consists of ESP32 for control, computer vision for gesture recognition, and voice commands for movement.

### üîπ System Workflow:
- Input: Gesture & Voice Commands
Camera captures hand gestures ‚Üí ESP32 processes via OpenCV + Mediapipe.
Microphone captures voice ‚Üí Converted into text using Edge AI Speech Recognition.
- Processing & Decision Making
ESP32 receives gesture/voice input, processes commands, and sends signals to motor drivers.
If no input is given, the system enters autonomous mode and follows a predefined path or avoids obstacles.
- Output: Car Movement & Obstacle Detection
Motor drivers control the wheels based on input.
Ultrasonic sensors detect obstacles ‚Üí ESP32 automatically stops or reroutes.
### üõ†Ô∏è Component List:
- 1Ô∏è‚É£ Microcontroller & Processing Unit
ESP32-CAM ‚Üí For gesture recognition & image processing.
ESP32 DevKit V1 ‚Üí Main control board for motor and sensor processing.
- 2Ô∏è‚É£ Input Components
OV2640 Camera Module ‚Üí Captures hand gestures.
MEMS Microphone (INMP441) ‚Üí Captures voice commands.
Ultrasonic Sensors (HC-SR04) ‚Üí Detects obstacles.
IR Sensors (Optional) ‚Üí For line-following capability.
- 3Ô∏è‚É£ Output Components
L298N Motor Driver ‚Üí Controls DC motors.
Two DC Motors ‚Üí Moves the toy car.
OLED Display (Optional) ‚Üí Displays detected gestures/commands.
- 4Ô∏è‚É£ Power & Connectivity
Li-ion Battery (7.4V 18650) ‚Üí Powers the entire system.
WiFi/Bluetooth Module (Built-in ESP32) ‚Üí For IoT connectivity (Firebase/MQTT).
### üì° Features & Functionality
- ‚úÖ Gesture-Controlled Mode:
Recognizes 5-6 hand gestures to move forward, left, right, stop, etc.
Uses Mediapipe Hand Tracking + ESP32-CAM for real-time control.
- ‚úÖ Voice-Controlled Mode:
Uses speech-to-text conversion for command execution.
Works offline using Vosk/Picovoice AI models.
- ‚úÖ Obstacle Avoidance Mode:
Ultrasonic sensors detect objects within 10-15 cm range.
Car stops or auto-navigates around obstacles.
- ‚úÖ Self-Navigation Mode (Future Upgrade)
Uses Reinforcement Learning (Q-learning/DQN) to follow a pre-defined path.
Line following (IR sensors) or GPS tracking (for real wheelchair).



